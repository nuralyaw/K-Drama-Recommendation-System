# -*- coding: utf-8 -*-
"""kdrama_recommendation_system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f9L5NU7ZZWzi5DjA__Vm7gjK5UffweA9

# 1. Import libraries and dataset
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import nltk
nltk.download('stopwords')
import string # Contains punctuation and other string aspects
from nltk.tokenize import word_tokenize
import re

nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

from sklearn.metrics import silhouette_score
from wordcloud import WordCloud

df = pd.read_csv("latest_dataset_2208")

"""# 2. EDA"""

# shape of dataset (rows, columns)
df.shape

# datatypes of each columns
df.dtypes

# drop unwanted columns
df.drop(columns= 'Unnamed: 0', inplace= True)

# number of unique values in each column
df.nunique()

# check for duplicated values
df.duplicated().value_counts()

"""# EDA - Visualisations"""

plt.figure(figsize = (9,4)) # setting the size of the output

sns.countplot( data = df,
             x = 'Score',

             color = 'orange') # setting the colours
            #order=df['Score'].value_counts().index) # https://www.statology.org/seaborn-countplot-order/
plt.xticks(rotation = 35)
plt.title('Ratings of K-Dramas')
plt.ylabel('Count of K-Dramas')
plt.xlabel('Rating')
plt.show()

"""### To provide better recommendations, a new dataset is created to include only dramas that has a score of 7.5 and above"""

filtered_df = df[df['Score'] >= 7.5]

plt.figure(figsize=(10, 5))

sns.countplot(data=filtered_df,
              y='Episodes',

              palette='summer_r',  # setting the colours
              order= filtered_df['Episodes'].value_counts().head(10).index)  # https://www.statology.org/seaborn-countplot-order/

plt.title('Top 10 Number of Episodes for Higher Scored Shows')
plt.ylabel('Number of Episodes')
plt.xlabel('K-Drama Count')
plt.show()

plt.figure(figsize=(10, 5))

sns.countplot(data=filtered_df,
              y='Genres',

              palette='summer_r',  # setting the colours
              order=filtered_df['Genres'].value_counts().head(10).index)  # https://www.statology.org/seaborn-countplot-order/

plt.title('Top 10 Genres in Higher Scored Shows')
plt.ylabel('Genres')
plt.xlabel('K-Drama Count')
plt.show()

plt.figure(figsize=(10, 5))

sns.countplot(data=filtered_df,
              y='Content Rating',

              palette='summer_r',  # setting the colours
              order=filtered_df['Content Rating'].value_counts().index)  # https://www.statology.org/seaborn-countplot-order/

plt.title('Content Rating of Higher Scored Shows')
plt.ylabel('Content Rating')
plt.xlabel('K-Drama Count')
plt.show()

"""# 3. Data Cleaning and Pre-Processing"""

df1 = filtered_df.copy()

# view kdramas that have no descriptions
df1[df1['Description'] == 'Unknown']

"""*   ### Dropping the kdramas with unknown descriptions as they have a low impact on the overall data.



"""

# listing the titles for future reference
titles_w_no_desc = list(df1.loc[df1['Description'] == 'Unknown', 'Title'].str.lower())

df1 = df1[df1['Description'] != 'Unknown']

df1[df1['Description'] == 'Unknown']

"""*   ### Creating a new column for clustering attrubutes





"""

df1['clustering_attributes'] = (df1['Description'] + ' ' + df1['Genres'] + ' ' + df1['Tags'])

# make column unicode
df1['clustering_attributes'].values.astype("U")

"""*   ### Removing unimportant words


1.   English stopwords from NLTK library
2.   Punctuations
3. Romanised Korean Syllables
4. Additional stopwords







"""

# common romanised korean syllables usually found in korean names
common_korean_names = 'dae hui seok beok yeo hyung san dam jong park chul geu roo shi hee eun yoo baek jae sang woo woong geon wook ho gi na sool seon goo kyung hyuk dong seung joon kwon shin hwa yeong seo yeon jun soon sung yang myung gil bin gwang hoon hae rin chan sook cha ho suh yul moo yung yong young chan  suk guk yul kim lee yi choi jung jeong kang cho jo yoon jang lim hong shik min hye jin soo hyun tae hyo han joo chae '

# making it into a list
common_korean_names = common_korean_names.split()
common_korean_names = [token.strip() for token in common_korean_names]

# utilising stopwords from the nltk library and adding other important words and punctuations to the list
stpwrd = nltk.corpus.stopwords.words('english')
additional_stopwords = ['unknown', 'also', 'lead', 'mydramalist', "n't", 'source', 'viki', 'asianwiki',
                        'mbc', 'kbs' ,'dramawiki', 'naver', 'hancinema', 'soompi', 'sbs', 'netflix',
                        'disney', 'jtbs', 'tvn']
stpwrd.extend(additional_stopwords)
stpwrd.extend(string.punctuation)
stpwrd.extend(common_korean_names)

# creating a new column - clustering attributes column tokenised
df1['tokens'] = df1['clustering_attributes'].apply(word_tokenize)

# stripping the tokens
df1['tokens'] = df1['tokens'].apply(lambda document : [token.strip() for token in document])

# creating a function to preprocess data
def prep_data(text):

    '''a function for removing the stopwords and lemmatizing the words'''

    # removing the stop words and lowercasing the selected words
    words  = [word.lower() for word in text if word.lower() not in stpwrd]

    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    # joining the list of words with space separator
    return words

df1['tokens'] = df1['tokens'].apply(prep_data)

# excluding words that have less than 3 characters
df1['tokens'] = df1['tokens'].apply(lambda document : [token for token in document if len(token)>2])

# creating a function to transform lists into a string

def listToString(s):
    ''' a function to join elements in a list into a single string'''
    # initialize an empty string
    str1 = " "

    # return string
    return (str1.join(s))

df1['tokens'] = df1['tokens'].apply(listToString)

df1['tokens']

"""## Vectorization


"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

vectorizer = TfidfVectorizer()
features = vectorizer.fit_transform(df1['tokens'])

features.shape

type(features)

features = features.toarray()

"""## Reducing dimensionality



*   Dimensionality reduction is used to simplify and improve the quality of data, making it more manageable for analysis and modeling



"""

from sklearn.decomposition import PCA

pca = PCA(random_state=42)
pca.fit(features)

# Explained variance for different number of components
plt.figure(figsize=(7,4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.title('PCA - Cumulative explained variance vs number of components')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')

"""*   ### More than 80% of variance is explained just by 750 components







"""

pca = PCA(n_components=750,random_state=42)
pca.fit(features)

features_pca = pca.transform(features)

features_pca.shape

"""# 4. K-Means Clustering



*   To group K-Dramas into clusters
*   Optimal number of clusters are to be decided by the elbow method and the silhouette coefficient

## Elbow Method
"""

# elbow method

distortions = []
K = range(1,30)
for k in K:
    kmeanModel = KMeans(n_clusters=k, random_state = 42)
    kmeanModel.fit(features_pca)
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

"""## Silhoutte Coefficient"""

# silhoutte coefficient

from sklearn.metrics import silhouette_score

k_range = range(2,30) ## Look up between K=2 and K=30 clusters
scores = [] ## Save the silhouette scores
inertia = [] ## Save the inertia scores

for k in k_range:
    ''' Run K-means models and store their respective inertia scores and silhouette scores '''
    km3 = KMeans(n_clusters=k, random_state= 42) ## Keep the random state constant!
    km3.fit(features_pca) ## Fit it on the scaled data
    scores.append(silhouette_score(features_pca, km3.labels_)) ## Append the silhouette
    inertia.append(km3.inertia_) ## Append the inert

plt.plot(k_range, scores)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.grid(True)

kmeans = KMeans(n_clusters=8, random_state=42)
kmeans.fit(features_pca)

kmeans_distortion = kmeans.inertia_
kmeans_silhouette_score = silhouette_score(features_pca, kmeans.labels_)

print((kmeans_distortion,kmeans_silhouette_score))

"""

*   ### Optimal number of clusters is 8
*   ### Silhoutte coefficient of 0.0135 is relatively low, suggesting that the custers are not well formed.


*   ### However, we shall continue with this value and allow room for improvement in the future




"""

df1['kmeans_cluster'] = kmeans.labels_

# plotting the number of kdramas in each kmeans cluster
plt.figure(figsize=(10,8))
q = sns.countplot(x='kmeans_cluster',data=df1, color = 'cornflowerblue')
plt.title('Number of K-Dramas in each cluster - Kmeans Clustering')
plt.xlabel('K-Means Cluster')
plt.ylabel('Count of K-dramas')
for i in q.patches:
  q.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

"""## Wordclouds for Each Cluster"""

def kmeans_worldcloud(cluster_num):

  ''' function to create wordclouds for each cluster'''

  comment_words = ''
  stopwords = stpwrd
  # iterate through the csv file
  for val in df1[df1['kmeans_cluster']==cluster_num].Description.values:

      # typecaste each val to string
      val = str(val)

      # split the value
      tokens = val.split()

      # Converts each token into lowercase
      for i in range(len(tokens)):
          tokens[i] = tokens[i].lower()

      # Filter out tokens with lengths less than or equal to 2
      filtered_tokens = [token for token in tokens if len(token) > 2]

      comment_words += " ".join(filtered_tokens)+" "

  wordcloud = WordCloud(width = 700, height = 700,
                  background_color ='white',
                  stopwords = stopwords,
                  min_font_size = 10).generate(comment_words)


  # plot the WordCloud image
  plt.figure(figsize = (10,5), facecolor = None)
  plt.imshow(wordcloud)
  plt.axis("off")
  plt.tight_layout(pad = 0)

for i in range(8):
  print('cluster:' + str(i) )
  kmeans_worldcloud(i)

"""## Plotting K-Means Clusters"""

y_kmeans = kmeans.fit_predict(features_pca)

num_clusters = 8

plt.figure(figsize=(13, 8))

for cluster_idx in range(num_clusters):
    plt.scatter(
        features_pca[y_kmeans == cluster_idx, 0],
        features_pca[y_kmeans == cluster_idx, 1],
        s=100,
        label=f'Cluster {cluster_idx + 1}',
        alpha=0.5
    )

plt.scatter(
    kmeans.cluster_centers_[:, 0],
    kmeans.cluster_centers_[:, 1],
    s=200,
    c='black',
    label='Centroids'
)

plt.title('Clusters of K-Dramas')
plt.legend()
plt.show()

"""# 5. Recommendation System"""

recommender_df = df1.copy()

recommender_df["Title"] = recommender_df["Title"].str.lower()

recommender_df.set_index('Title',inplace=True)

indices = pd.Series(recommender_df.index)
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

def recommend_5():

    ''' a function to recommend users similar k-dramas to the ones they have inputted'''

    while True:

      print('Please enter the K-Drama you want recommendations on or enter -1 if you would like to cancel.')
      title = input()
      if title == '-1':
        break
      title_lower = title.lower()
      if title_lower in titles_w_no_desc:
        print('Sorry, we do not have enough information on this K-Drama. Please input a different title.')
      else:
        try:
            recommend_content = [] # empty list for recommended titles

            input_cluster = recommender_df.loc[title_lower, "kmeans_cluster"]

            CV = CountVectorizer()
            converted_matrix = CV.fit_transform(recommender_df['tokens'])
            cosine_sim = cosine_similarity(converted_matrix)

            idx = recommender_df.index.get_loc(title_lower)
            series = pd.Series(cosine_sim[idx])
            similarities_with_cluster = zip(series, recommender_df.index, recommender_df["kmeans_cluster"])
            sorted_similarities_with_cluster = sorted(similarities_with_cluster, key=lambda x: x[0], reverse=True)

            # Appending dictionaries with movie information from the same cluster
            for similarity, kdrama_index, cluster in sorted_similarities_with_cluster:
                if cluster == input_cluster and kdrama_index != title_lower:
                    recommended_movie = {
                        "Title": kdrama_index.title(),
                        "Description": recommender_df.loc[kdrama_index, "Description"],
                        "Genre": recommender_df.loc[kdrama_index, "Genres"],
                        "Tags": recommender_df.loc[kdrama_index, "Tags"],
                        "URL": recommender_df.loc[kdrama_index, "URL"],
                        "Similarity": similarity,
                        "Cluster": recommender_df.loc[kdrama_index, "kmeans_cluster"]
                    }
                    recommend_content.append(recommended_movie)

            # Display recommendations
            print(f"If you liked '{title.title()}', you may also enjoy these: \n")
            recommended_df = pd.DataFrame(recommend_content)
            return recommended_df.head(5)

        except:
            print('Invalid entry. Movie title not in system. Please input a different title')

recommend_5()